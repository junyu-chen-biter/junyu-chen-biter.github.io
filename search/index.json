[{"content":"kaggle入门笔记 下载数据 首先李沐的那个一大推函数不要学，没用，直接在官网上下载数据，命名后再在同一个文件夹下，然后一般使用代码：\n1 2 3 4 # 读取训练集数据，路径为 \u0026#34;train.csv\u0026#34; train = pd.read_csv(\u0026#34;train.csv\u0026#34;) # 读取测试集数据，路径为 \u0026#34;test.csv\u0026#34; test = pd.read_csv(\u0026#34;test.csv\u0026#34;) 常见的损失函数简介 nn.MSELoss()\n均分误差，一般用于回归问题\nnn.BCELoss()\n二元交叉熵函数，使用的时候必须用sigmoid激活函数，用于二分问题\nnn.nn.CrossEntropyLoss()\n交叉熵函数，用于多分类问题\n以李沐的房价为例的分析 首先明确几个点，这些问题是因为这个代码的特殊性而没有遇到的\n房价预测是一个回归问题，回归问题套用这个模板是没啥问题的\n李沐的模型中没有引用dropout等模块，所以测试时没有开启评估模式和禁止梯度\n数据处理 train数据是一个由字符类型和数值类型两种量的数据，每一行代表和一个数据\n先pd.read_csv(path),一般train和test都要read出来 然后concat函数，合并test与train，这个时候选择性的去除一些无关变量 对数据进行处理 面对既有数据又有类型的东西，先是提取了提出数据为数值的列，然后进行了归一化,然后把缺失值填冲为0（根据需求选择怎么填充）\n1 2 3 4 5 6 numeric_features = all_features.dtypes[all_features.dtypes != \u0026#39;object\u0026#39;].index all_features[numeric_features] = all_features[numeric_features].apply( lambda x: (x - x.mean()) / (x.std())) all_features[numeric_features] = all_features[numeric_features].fillna(0) 之后是对非数值数据处理的关键，转化为热独编码\n1 all_features=pd.get_dummies(all_features,dummy_na=True) 大多数机器学习算法只能处理数值型数据，而不能直接处理分类数据。分类数据（如颜色：红、绿、蓝；城市：北京、上海、广州）没有数值上的大小或顺序关系，如果直接将它们映射为简单的整数（如红 = 1，绿 = 2，蓝 = 3），算法可能会错误地认为这些数值之间存在大小或顺序关系，从而影响模型的性能。 独热编码将每个分类变量转换为一个二进制向量，每个向量的长度等于该分类变量的不同取值的数量。每个取值对应向量中的一个位置，取值出现时该位置为 1，否则为 0。这样可以避免算法错误地理解分类变量之间的关系。\n之后就是将这些数据转化为pytorch张量，这样之后才可以进行训练，因为torch框架用的就是张量，这样才可以自动求导\n1 2 3 4 5 6 7 8 9 10 11 #先转化为np的数据 all_features = all_features.astype(np.float32) #提取这个数据的大小，取行列 n_train=train_data.shape[0] #.value就是转化为numpy数组，这里涉及到tensor的用法 train_features=torch.tensor(all_features.iloc[:n_train].values,dtype=torch.float32) test_features=torch.tensor(all_features.iloc[n_train:].values,dtype=torch.float32) #这里的labels就是目标变量的意思，这里是房价 train_labels=torch.tensor( train_data.SalePrice.values.reshape(-1,1),dtype=torch.float32 ) 定义一些工具 经典的就是李沐的在评估预测效果时，使用的是 log_rmse 函数计算的，题目要求，而这个函数就是自己手写的，包装在一个函数里面，\n1 2 3 4 5 #计算均方根误差 def log_rmse(net,features,labels): clipped_preds=torch.clamp(net(features),1,float(\u0026#39;inf\u0026#39;)) rmse=torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels))) return rmse.item() #.item就是转化为真值类型 写出这样的工具也是十分的有难度，可以借助ai工具，\n而损失函数，则可以直接由torch内置的库来调用 loss=nn.MSELoss()\r当然其实不止这一种损失函数，自己看着用就行\n开始训练 李沐将train函数写出来之后内置到了k折交叉验证之中，所以对于初学者来说看起来会十分麻烦。 同时他直接用了K折交叉验证，无异于加大了初学者的理解难度 首先解释get_k_fold_data这个函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_k_fold_data(k, i, X, y): # 这行代码使用了断言（assert）技术。 # 断言是一种调试工具，用于确保某个条件为真。 # 如果 k 不大于 1，程序会抛出 AssertionError 异常并终止。 # 因为 k 折交叉验证要求至少将数据分为两部分，所以 k 必须大于 1。 assert k \u0026gt; 1 # 这行代码计算每一折的样本数量。 # X.shape[0] 表示特征矩阵 X 的行数，即样本的总数。 # 使用整除运算符 // 确保结果为整数，得到每一折大致相等的样本数量。 fold_size = X.shape[0] // k # 初始化训练集的特征和标签为 None。 # 在后续循环中，我们会逐步构建训练集。 X_train, y_train = None, None # 这是一个 for 循环，用于遍历 k 个折。 # j 是循环变量，从 0 到 k - 1 依次取值。 for j in range(k): # 这行代码创建了一个切片对象 idx。 # slice 函数用于生成一个切片对象 # 它指定了从 j * fold_size 到 (j + 1) * fold_size 的索引范围。 # 这个索引范围对应了第 j 折的数据在整个数据集中的位置。 idx = slice(j * fold_size, (j + 1) * fold_size) # 这行代码根据切片对象 idx 从特征矩阵 X 和标签向量 y 中提取第 j 折的数据。 # X[idx, :] 表示提取特征矩阵 X 中索引范围为 idx 的所有行，所有列的数据。 # y[idx] 表示提取标签向量 y 中索引范围为 idx 的数据。 X_part, y_part = X[idx, :], y[idx] # 这是一个条件判断语句。 # 如果当前折的索引 j 等于指定的验证折索引 i，说明这一折将作为验证集。 if j == i: # 将第 j 折的数据作为验证集的特征和标签。 X_valid, y_valid = X_part, y_part # 如果训练集还未初始化（即 X_train 为 None），说明这是第一次添加数据到训练集。 elif X_train is None: # 将第 j 折的数据作为训练集的特征和标签。 X_train, y_train = X_part, y_part # 否则，说明训练集已经有部分数据，需要将当前折的数据添加到已有的训练集中。 else: # 这行代码使用了 torch.cat 函数，它用于将多个张量沿着指定的维度拼接在一起。 # [X_train, X_part] 是要拼接的张量列表，0 表示沿着第 0 维（样本维度）进行拼接。 # 这样就将当前折的特征数据添加到了训练集的特征数据中。 X_train = torch.cat([X_train, X_part], 0) # 同理，将当前折的标签数据添加到了训练集的标签数据中。 y_train = torch.cat([y_train, y_part], 0) # 最后，返回训练集的特征、训练集的标签、验证集的特征和验证集的标签。 return X_train, y_train, X_valid, y_valid 言简意赅的就是，K折交叉验证是将一个完整的数据分为k批次，然后依次选一个为验证集，其他为训练集（这就是为什么这个方法一定要写for循环），这个函数的的作用，就是提取出了哪些为验证级，哪些为数据集。最后返回训练集的特征、训练集的标签、验证集的特征和验证集的标签。\n然后看看train函数，train函数最具有借鉴价值，其中的许多结构都可以照搬不误\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size): # 初始化两个空列表，用于存储每个训练周期（epoch）结束后的训练损失和测试损失。 # 这些损失值将以对数均方根误差（log RMSE）的形式记录。 train_ls, test_ls = [], [] # 使用 d2l.load_array 函数创建一个训练数据的迭代器 train_iter。 # 该函数将训练特征和训练标签组合成一个数据集，并按照指定的批量大小进行划分。 # 这样，在训练过程中可以按批次加载数据，提高训练效率。 train_iter = d2l.load_array((train_features, train_labels), batch_size) # 使用 torch.optim.Adam 优化器来更新模型的参数。 # Adam 是一种常用的优化算法，结合了 AdaGrad 和 RMSProp 的优点。 # net.parameters() 表示要更新的模型参数，lr 是学习率，weight_decay 是权重衰减系数。 optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay) # 外层循环，控制训练的轮数，即模型对整个训练数据集进行多少次完整的训练。 for epoch in range(num_epochs): # 内层循环，遍历训练数据的每个批次。 # 在每个批次中，从 train_iter 中获取一批特征数据 X 和对应的标签数据 y。 for X, y in train_iter: # 在每次参数更新之前，需要将优化器中的梯度清零。 # 这是因为 PyTorch 会累积梯度，不清零会导致梯度错误地累加。 optimizer.zero_grad() # 前向传播：将输入数据 X 传入模型 net 中，得到模型的预测值。 # 然后使用预先定义的损失函数 loss 计算预测值与真实标签 y 之间的损失。 l = loss(net(X), y) # 反向传播：计算损失函数关于模型参数的梯度。 # 这一步会自动计算并存储每个参数的梯度值。 l.backward() # 优化器根据计算得到的梯度更新模型的参数。 # 具体的更新规则由优化器的算法决定，这里使用的是 Adam 算法。 optimizer.step() # 在每个训练周期结束后，计算并记录训练集的对数均方根误差（log RMSE）。 # log_rmse 函数会将模型的预测值进行裁剪，避免出现负数或零，然后计算对数均方根误差。 train_ls.append(log_rmse(net, train_features, train_labels)) # 如果提供了测试数据的标签，则计算并记录测试集的对数均方根误差（log RMSE）。 if test_labels is not None: test_ls.append(log_rmse(net, test_features, test_labels)) # 最后返回训练集和测试集（如果有）在每个训练周期结束后的对数均方根误差列表。 return train_ls, test_ls 对于这个函数，我们总结这样的套路\ntrain_iter必不可少\ntrain_iter=d2l.load_array((train_features,train_labels),batch_size) 这一行姑且看作是固定的代码，有了合格datalodar的数据类型，才方便进行进一步的分组训练\n但是你以后是肯定不用d2l库的（这就是一个大槽点），所以用pytorch自己的库文件就给这样写\n1 2 3 4 5 6 7 8 from torch.utils.data import TensorDataset, DataLoader # 原代码 # train_iter = d2l.load_array((train_features, train_labels), batch_size) # 替换后 train_dataset = TensorDataset(train_features, train_labels) train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) 选择一个优化器\noptimizer=torch.optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n建立for循环训练 其实如何更新参数的具体过程，已经内置在了torch里面，这里其实是看不见的 X是特征，y是标签\nfor epoch in range(num_epochs):\rfor X,y in train_iter:\roptimizer.zero_grad()\rl=loss(net(X),y)\rl.backward()\roptimizer.step()\rtrain_ls.append(log_rmse(net,train_features,train_labels))\rif test_labels is not None:\rtest_ls.append(log_rmse(net,test_features,test_labels))\r此函数最后放回的是训练集和测试集的均方根误差，用于衡量这个效果 同时net也训练好了，优化器自动做好了这些事情\nk_fold函数 解释了上面两个函数之后，k_fold函数就不难了，返回的是k次验证的平均损失值，这里不再讲解\ntrain_and_pred函数 简称来说，这一步就是在之前已经完成了net的训练之后，再一次传入test_data，然后回放回相应的标签 但是要注意，这里这个方法是预测出的价值，只用于回归问题，不可以用于分类问题 preds = net(test_features).detach().numpy()\r这里detch方法和numpy都直接计吧 如果要用分类问题的话，对于二分类，可以用 preds_bool = preds \u0026gt;= 0.5\r最后保存成csv文件提交就好了\n1 2 3 test_data[\u0026#39;SalePrice\u0026#39;] = pd.Series(preds.reshape(1, -1)[0]) submission = pd.concat([test_data[\u0026#39;Id\u0026#39;], test_data[\u0026#39;SalePrice\u0026#39;]], axis=1) submission.to_csv(\u0026#39;submission.csv\u0026#39;, index=False) ","date":"2025-07-15T00:00:00Z","permalink":"http://localhost:1313/p/kaggle%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"kaggle学习笔记"},{"content":"关于yaml的配置 这个文件可以修改网站的一些配置\ncopyright就是人的名字\n语言里面的title是我的名字\ncustomText是自定义文本\nenabled是评论，我其实先关掉了\nwidgets是装饰，可以自己选择关不关\nsocial是侧边栏的搜索，可以自己改\n在cmd中用命令，\nhugo new content post/博客名/index.md\n这样来创建内容\n在themes的stack里面有个样例，那里面可以有一些样例文件，可以查看 还有title是在文件里面直接改的\n还有，如果要在同一文件下引用图片，md文件的名字只能命名为index\n1 2 3 4 5 6 7 8 title: Chinese Test description: 这是一个副标题 date: 2020-09-09 slug: test-chinese image: helena-hertz-wWZzXlDpMog-unsplash.jpg categories: - Test - 测试 这上面就是样例可以改的地方，其中categories的地方就是旁边的标签，img是封面\n这个方法中。draft的状态还会影响是否显示，乐了\n目前网站还是手动部署，每一次写完以后，用hugo -D（在dev目录下）重新生成文件，之后进入public文件，\ngit add . git commit -m \u0026ldquo;更新xx年xx日\u0026rdquo; git push origin master\n然后在github上pull resquest，合并更新\n这个方法非常愚蠢，不是很好，因为有几次失误的操作导致我的main分支不知道去哪了（）\n","date":"2025-07-06T16:14:48+08:00","permalink":"http://localhost:1313/p/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%E8%AF%B4%E6%98%8E/","title":"网站搭建说明"},{"content":"京西南 大一小记 时间真是太快了，没想到短短一年的时间，大一就结束了，最后取得成绩还行，写此文的时候第二学期的成绩还没出来，所以在第一学期综测rank5%，纯成绩rank10%，第二学期纯纯被淑芬被刺，总rank没出来，但是应该不如上学期（），在这里写一些我踩过的坑，供后人参考，当然其中也有我不切实际的幻想，唉唉等未来再看吧\n题目名意为：北京西南边的良乡大一小记\n学习篇 推荐几个网站：\nhttps://www.tboxn.com/sites/320.html\nhttps://survivesjtu.gitbook.io/survivesjtumanual\n数学分析1 大一上学期最重要的课，十分偏证明。大一的课程很多，而且有军事理论军事技巧，并且24届批分出来之后分差不大，没出现5%比20%高10多分的情况，所以尽量好好学，保障在前25%就不会拉总均分。我在学这个的时候，依然在保留纸质学习，真是一个大坑，尽早用无纸化学习更好\n线性代数 长远来看，这是大一最重要的课，然而我炸了，十分不适应老师的讲课风格，去找了一个全英文的网课看，而却完全忽略了课本，最后只考了一个均分，分析下来是计算太差了，同时不能完全不看课本。最后挺好应试的，但是只学到应试那个难度的话，对以后的科研没啥帮助，我就后悔一直以一个应试的态度学这门课，平时也是只做好课后习题。线性代数完全值得你去找不同的课本反复学习多遍，理解其中的原理是最好的。\nC语言 很看发挥啊，主要就是要提前备考期末会考的那几个算法，递归、链表、字符串、打印图形。其实挺没意思的，这么考最后比的是应试能力，一堆oi算法都没涉及，会导致很多学生平时的努力无法反映在考试中，但是可以借此探索一下自己的编程水平。我其实考寄了，但是老师大捞特捞，最后评分也不差\n人工智能and生科 放在一起说明这两个课就是纯纯的背书，平时一点不用听，考前看PPT和资料完全足够。人工智能这个课，虽然吐槽的很多，但是客观上来说确实讲了许多ai的知识，是一个较为广泛的科普\n思政and习概and史纲 这个主要看老师和平时分，想卷高分平时分一定要拉满！考试的话，思政看小红书的知识点，习概看老师的重点，史纲emmmm看命。\n数学分析2 真正的淑芬王朝！，大一下想要均分高，淑芬二是关键中的关键！！！因为分差极大。我大一下每一科的排名，综合来看是要比大一上好（因为大一上线代寄了），但是淑芬二却让我总排名不如大一上。就是因为大一下的学分少，淑芬占比高而且分差大，其他科目难以拉开分，我74分，均分64，然而随便来一个淑芬上80的佬，就可以给我拉开一大堆均分。大一下的总成绩排名，也是和淑芬高度相关。如果想卷，这个科目甚至可以寒假先开始学\n大物1 没啥可说的，纯纯老本，我觉得定期复习加上总结一下公式就行了，可以照搬高中的方法，难度不大，但是4学分，还是尽量冲高分\n机械制图 纯纯折磨，24届的几位伟大学长给小灯制作了完整的题库，非常有用。这个科目，基本就是看PPT加刷题，平时的作业一定好好做。至于听课，因人而异吧。\n智能机电实践 队友大于一切，本人幸运地遇见大佬队友，最后97分拿下此课，没啥技巧，祈祷自己遇见好的队友吧。哦哦，这个课程很锻炼ai使用能力，如果愿意深入了解，其实是一个很好的增强自己实践能力的契机，也是一个很好的偏自动化控制算法的入门，态度好是真能学到东西\n大学物理实验 难评价，给分有点看运气，想要高分，尽量确保自己绪论部分拿满，那是真的自己可以改变的部分\n课外事物 大创 其实可以算参加了两个，但是都不是核心成员，而且成绩也未知。这个东西，想搞的话就去跟老师（然后你就会被国内高校界的挂名包装之风污染灵魂）。很难评，有些大创真的做东西，比赛过程中甚至会产出论文和专利，有的就是纯纯做PPT。所以建议找老师做，老师会提供一些资源。但是很多大创都是把老师的项目套给学生，这就看自己能不能接受了\n比赛 算法类的唯一算得上成就的是PAT三等奖（好吧，也是水赛），oi类的比赛对于无编程基础的小灯基本可以劝退。含金量高、无挂名可能的背后，也是其很低的收益付出比，非oier想卷计算机还有很多其他赛道可以选择\n建模类无，参加了一个统计建模，了解了大概是个啥流程，之后感觉不感兴趣，就没有坚持。个人觉得含金量一般（包括国赛），没听见在什么地方这个比赛是很必要的\nrm，rc无，我就没参加这些队伍，乐。\n学生事物 组织加了个记者团和学生会，社团是辩论社。大一上确实花了些时间在辩论上，最后拿了院级亚军，感觉辩论给人的影响还是正面的，也可以认识许多人，是段不错的经历。学生组织的话也主要是认识了一些朋友，不过我大一下就基本没弄这些了\n班委当的是班长，相比于团支书，我真是十分悠闲的了\n实习and科研 zero，彻底没开始\n推荐篇 《金榜题名之后：大学生出路分化之谜》这个书值得一看，至少给了我很多启发\n纯懒，慢慢更新\n","date":"2025-07-06T00:00:00Z","image":"http://localhost:1313/p/%E4%BA%AC%E8%A5%BF%E5%8D%97%E5%A4%A7%E4%B8%80%E5%B0%8F%E8%AE%B0/farm_hu_4b05d451a45eb568.jpeg","permalink":"http://localhost:1313/p/%E4%BA%AC%E8%A5%BF%E5%8D%97%E5%A4%A7%E4%B8%80%E5%B0%8F%E8%AE%B0/","title":"京西南大一小记"},{"content":"这是音乐 其实也只有自己能懂，那些音乐背后的回忆才是最珍贵的，这东西也不能分享，于是写在此处（暴露乐品）\n于是这便是一章随机，听到啥想记的，就随手记下来吧，可以也就当作一个音乐封面的记录好了\n2025-7-6，于假期，考科目三前（最火考过了）\n2025-7-6，于假期，搭建网站时\n","date":"2025-07-06T00:00:00Z","image":"http://localhost:1313/p/%E9%82%A3%E4%BA%9B%E6%AD%8C%E6%9B%B2/song1-unsplash_hu_f3664672da805c1a.jpg","permalink":"http://localhost:1313/p/%E9%82%A3%E4%BA%9B%E6%AD%8C%E6%9B%B2/","title":"那些歌曲"}]