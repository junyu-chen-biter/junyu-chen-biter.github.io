[{"content":"原本计划是在期末考完再提笔写这个总结，但是今天是2025年的12月31日，忽然有感，于是就像先表达一些什么\n今年的主基调我想是失败的，失败在哪呢？我想是迷茫 今年作为旁观者，看了很多他人的成功，从数学建模国赛省赛，世纪杯，igem，数学竞赛，物理竞赛，到年末最后一天出成绩的雷达挑战赛…… 这些比赛，都没有我的名字，一种伤心和难过席卷了现在的我，为何在这个成功渐渐冒头的2025年，我如此不堪呢，说说我的心里话：\n今年是追求最佳路径的一年，但是大一大二的学生无法做出最佳的判断，总是在事情开始前就把这个事计划为了会挫败自己做其他事情，也总是假设不做这件事情我就可以把手上的事情弄得更好\n我于2025年的一月结束了期末考试，在寒假没有推进自己的大创项目，因为说实话，当时我啥技术都不懂，也不知道该学什么，大一上学期没有认识yht和ll这样的开发大佬，也不知道自己以后的方向，于是假期花时间装模做样的弄matlab建模学习，但是最后我都没有使用上这个技能，纯纯浪费了时间，于是假期的大部分时间是在弄三角洲行动，学驾照，不知道未来的玩乐。\n那个时候参加了acm的寒训，但是我最后放弃了，确实也应该放弃，一度认为自己不会选择计算机专业，（似乎说明这次经历没有改变我的专业选择），这个假期在acm寒训上花的时间也是完全荒废了，还有就是在尝试弄igem的一个招新的项目，但是最后无功而反，那个时候的我确实是弄不出来这个东西的，而且同时，我对参加这个比赛的决心动摇了，我觉得自己未来的方向和这个比赛无关。目前来看，这个比赛获得了国际级的银奖，我错过了很大的奖项，但我确实放弃了……………………\n收假后，尝试去弄了PAT考试，这个比赛没啥含金量其实，不过算是对acm寒训的补偿\n到了4月份，一件改变了2025年全部基调的事情发生了，我想尝试去进入老师课题组打挑战杯比赛，那时的我老实本分，联系了光电学院的一个老师，但是那个老师没有给我任何的教学，而是想让我直接去挂名参赛，甚至想要我去作为负责人申请一个比赛。当然，现在看来这也是完全不负责任的操作，只能说明这个老师的课题组实在是没有一个可用的本科生了，因为这样的步步紧逼，我最后决定彻底放弃这个比赛，于是原本想全心投入比赛的大一下学期就变成了学技术和卷rank，后来发现这个老师确实是一个“十分雷”的老师，这次是狠狠的吃了信息差的亏\n后来的五月和六月，我开始自学深度学习，但是因为是自学，我的学习效果十分有限，只看书其实效果也十分有限，至少我之前没有这样自学的经历，最终自学的效果只能说是十分的差，但是这却是我实实在在花了大量时间的地方\n同时在大一下还有统计建模比赛，这本来就是一次机会，如果我站出来，整合团队，带领团队，那么其实是可以有好的结果的，我们的队伍不差，但是那个时候我似乎没有意识到这个比赛的重要性，是的，我摆烂了，整整一个多月，我们组毫无进展，原本是一个很好的学习机会，我自己浪费了，这是我的问题，我明明有时间去弄好这个比赛的啊，但是就是觉得就算打的好，这个比赛也没啥用啊，还不如摆了，课内的绩点更重要，如果一个人就是用这样的态度去经历自己的大学，他真的会成功吗？这是一个疑问句，因为我不知道答案\n期末的时候，一个学长联系我又去加入一个大创，我接受了，活从假期开始，只用写写文书，我原本以为这是一个绝佳的躺着获奖的机会，但是事实是NO！这个比赛最后无功而返，我大二上的综测因此陷入了严重的不足中\n暑假我还是想学技术，但是明显三角洲成了拦路虎，确实，我假期打了太多的游戏了，我失败了，我依旧执着于深度学习的学习和实战，重点是实战，我想用深度学习做项目。但是没有适合的教程，B站上确实没有适合的教程，我找了好久，装模做样的复制粘贴了几个项目，但是整体对于实力的提升没有多大，可以说一整个暑假都是荒废了，这算是方向的错误，一个没有接触过工业界和科研的学生去自学这些看似为科研准备的东西，本身就是不正确的，可笑的，暑假算是我决策的巨大失误\n我原本打算放假的时候就去找个老师进组，但是十分不幸的是我一直在拖，有一定的顾虑是自己啥都不会，想暑假再学点东西（但是你都没进组咋知道学什么），如果当初我确实刚放假就进组的话，可能结局会比现在好很多呢\n暑假了，最终还是做出了选专业的决定，目前来看，选择计算机科学与技术拔尖班是一个正确的决定，因为遇上了wby，yht，ll这样的同学，给了我很多启发，渐渐可以说是指明了我未来该走的路\n在假期的末尾，我参加了马蹄杯，这算是我一直水逆的一个奖励，因为我拿到了一个很水的省二，而且是可以被广泛认可的省二，这算是一个小小的回报吧\n大二上开始了，明显，国赛数学建模我没有参加，同样的原因：感觉这个比赛没用。 我在9月初也终于找了一个导师进组，可惜的是我最想干活的日子是9-11月这期间，但是这段时间确实没有任何的活，我接手的是一个全新的项目，学长那边确实需要时间处理，我那个时候也在看论文，但是这些论文基本是看了就忘，让一个初学者记住什么确实不容易，最终就是差不多两个月的时间都在看，没有干活，甚至一度怀疑想要退组。最后活终于下来了，但是很不巧，那个时候我也要期末了，能分的时间也不多了，所以也是仓促收尾\n假期那个学长的大创还在进行，不过变成了去弄一个agent项目，可以说是毫无产出，但是确实可以学到东西的玩意，但是我因为各种事情加深，没有花太多的时间看这个，后来听说我们的比赛啥结果都没有的时候，也是两眼一黑，觉得心血都白费了\n我还有一个主心骨是想备考六级，我确实启动的很早，在考六级前的那个周也分了很多的时间看六级，甚至把数值分析都摆了，但是最后考的状态十分差，我可以肯定我考六级是考批了，因为我做往年的模拟卷没有这么差劲，但是我觉得一定程度上，也是因为我没有在之前去完整的模拟六级的考试，正真上了考场，时间的把握就出了问题。可以总结下来，我的六级之路是失败了，但是我为了确实花了不小的成本，放弃了一些比赛等等，可是结果没有因为我放弃了这些东西而变好。原因我想是因为我没有从开学就坚持每天背单词，看似是花了很多时间卷，实则效率不高，该坚持的东西没有坚持，这是我自己的惰性的体现，不能怪其他外部因素\n从大一开始立项，持续一年的大创终于结束了，总结来说，我没有为这个大创全力以赴，我以为自己还有机会，于是我从来没有把他考虑的十分重要，确实，这个比赛获奖的概率不大，因为只是靠一个学长带队，而且主要的方向不是我负责的部分，理性来说，这个大创选择性放弃是正确的，而且因为参加了这个比赛，我没有在大一去以进组的方式参加其他老师的大创，可以说是浪费了一个很好的机会，不过我还是要感谢带我的学长。\n11月有全国大学生数学竞赛，这个比赛我确实是没啥水平获奖的，没有备赛，大部分知识都忘了，于是最终也是啥奖没有，变成了陪跑者，如果是我大一下的水平，我应该可以省三，但是世界上没有如果 12月有大学生物理竞赛，我复习了不到一个小时，目标是冲一个加分，两分的加分，但是最后在考场上，单摆的公式竟然忘却了如何推导，填空题尽力了，气压部分还是忘记咋做了，结局是啥奖没有，得知消息的时候十分伤心\n跨年的时候看见wyf去参加了极客松的跨年挑战赛，突然发现一年的时候真的可以改变太多，真的有很多的同学在2025是拼命的学，但是我没有，一阵阵的苦涩在我的心中徘徊，我前18年的人生似乎没有遇到这样的事情，我的未来会何去何从呢，我想这是一个巨大的问题了，我能不能找到那个让我彻底改变想法的理由呢？\n这两个比赛，都可以总结为我的遗忘曲线的问题，这些知识点全部忘记了，那谁来都考得不会好了，唉唉，是不是当初要是学的是高数我就会更好呢，我不知道，但是总而言之，这个比赛说明了我确实很大一部分课程内容是没有学好的，但是我的应试技巧不错，于是考的又不是太差。\n现在是2025的最后一天了，我似乎确实需要重新启航了。2025是很失败的一年，我始终在犹豫和徘徊，我始终没有不计后果的踏出一步，我始终觉得自己时间还多，同时我也自己堕落，我在学期内不够自律，在学期外沉迷游戏\n2025，就当作是人生休息的一年吧，就当作是人生停下来买感悟的一年吧，就当作是真正高考后面向社会寒意的一年吧，未来还会有很多困难，但是会相信自己是可以上桌的那一个\n正确的脚步不是选择出来的，而是要让选择变得正确，我想这就是我的2025没有想明白的道理吧\n2026，要重新启航了 ","date":"2025-10-06T00:00:00Z","image":"https://junyu-chen-biter.github.io/p/2025%E6%80%BB%E7%BB%93/memories_hu_f14dcd477a3bf1d3.jpg","permalink":"https://junyu-chen-biter.github.io/p/2025%E6%80%BB%E7%BB%93/","title":"2025总结"},{"content":"kaggle入门笔记 下载数据 首先李沐的那个一大推函数不要学，没用，直接在官网上下载数据，命名后再在同一个文件夹下，然后一般使用代码：\n1 2 3 4 # 读取训练集数据，路径为 \u0026#34;train.csv\u0026#34; train = pd.read_csv(\u0026#34;train.csv\u0026#34;) # 读取测试集数据，路径为 \u0026#34;test.csv\u0026#34; test = pd.read_csv(\u0026#34;test.csv\u0026#34;) 常见的损失函数简介 nn.MSELoss()\n均分误差，一般用于回归问题\nnn.BCELoss()\n二元交叉熵函数，使用的时候必须用sigmoid激活函数，用于二分问题\nnn.CrossEntropyLoss()\n交叉熵函数，用于多分类问题\n以李沐的房价为例的分析 首先明确几个点，这些问题是因为这个代码的特殊性而没有遇到的\n房价预测是一个回归问题，回归问题套用这个模板是没啥问题的\n李沐的模型中没有引用dropout等模块，所以测试时没有开启评估模式和禁止梯度\n数据处理\ntrain数据是一个由字符类型和数值类型两种量的数据，每一行代表和一个数据\n先pd.read_csv(path),一般train和test都要read出来 然后concat函数，合并test与train，这个时候选择性的去除一些无关变量 对数据进行处理 面对既有数据又有类型的东西，先是提取了提出数据为数值的列，然后进行了归一化,然后把缺失值填冲为0（根据需求选择怎么填充）\n1 2 3 4 5 6 numeric_features = all_features.dtypes[all_features.dtypes != \u0026#39;object\u0026#39;].index all_features[numeric_features] = all_features[numeric_features].apply( lambda x: (x - x.mean()) / (x.std())) all_features[numeric_features] = all_features[numeric_features].fillna(0) 之后是对非数值数据处理的关键，转化为热独编码\n1 all_features=pd.get_dummies(all_features,dummy_na=True) 大多数机器学习算法只能处理数值型数据，而不能直接处理分类数据。分类数据（如颜色：红、绿、蓝；城市：北京、上海、广州）没有数值上的大小或顺序关系，如果直接将它们映射为简单的整数（如红 = 1，绿 = 2，蓝 = 3），算法可能会错误地认为这些数值之间存在大小或顺序关系，从而影响模型的性能。 独热编码将每个分类变量转换为一个二进制向量，每个向量的长度等于该分类变量的不同取值的数量。每个取值对应向量中的一个位置，取值出现时该位置为 1，否则为 0。这样可以避免算法错误地理解分类变量之间的关系。\n之后就是将这些数据转化为pytorch张量，这样之后才可以进行训练，因为torch框架用的就是张量，这样才可以自动求导\n1 2 3 4 5 6 7 8 9 10 11 #先转化为np的数据 all_features = all_features.astype(np.float32) #提取这个数据的大小，取行列 n_train=train_data.shape[0] #.value就是转化为numpy数组，这里涉及到tensor的用法 train_features=torch.tensor(all_features.iloc[:n_train].values,dtype=torch.float32) test_features=torch.tensor(all_features.iloc[n_train:].values,dtype=torch.float32) #这里的labels就是目标变量的意思，这里是房价 train_labels=torch.tensor( train_data.SalePrice.values.reshape(-1,1),dtype=torch.float32 ) 定义一些工具\n经典的就是李沐的在评估预测效果时，使用的是 log_rmse 函数计算的，题目要求，而这个函数就是自己手写的，包装在一个函数里面，\n1 2 3 4 5 #计算均方根误差 def log_rmse(net,features,labels): clipped_preds=torch.clamp(net(features),1,float(\u0026#39;inf\u0026#39;)) rmse=torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels))) return rmse.item() #.item就是转化为真值类型 写出这样的工具也是十分的有难度，可以借助ai工具，\n而损失函数，则可以直接由torch内置的库来调用 loss=nn.MSELoss()\r当然其实不止这一种损失函数，自己看着用就行\n开始训练\n李沐将train函数写出来之后内置到了k折交叉验证之中，所以对于初学者来说看起来会十分麻烦。 同时他直接用了K折交叉验证，无异于加大了初学者的理解难度 首先解释get_k_fold_data这个函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_k_fold_data(k, i, X, y): # 这行代码使用了断言（assert）技术。 # 断言是一种调试工具，用于确保某个条件为真。 # 如果 k 不大于 1，程序会抛出 AssertionError 异常并终止。 # 因为 k 折交叉验证要求至少将数据分为两部分，所以 k 必须大于 1。 assert k \u0026gt; 1 # 这行代码计算每一折的样本数量。 # X.shape[0] 表示特征矩阵 X 的行数，即样本的总数。 # 使用整除运算符 // 确保结果为整数，得到每一折大致相等的样本数量。 fold_size = X.shape[0] // k # 初始化训练集的特征和标签为 None。 # 在后续循环中，我们会逐步构建训练集。 X_train, y_train = None, None # 这是一个 for 循环，用于遍历 k 个折。 # j 是循环变量，从 0 到 k - 1 依次取值。 for j in range(k): # 这行代码创建了一个切片对象 idx。 # slice 函数用于生成一个切片对象 # 它指定了从 j * fold_size 到 (j + 1) * fold_size 的索引范围。 # 这个索引范围对应了第 j 折的数据在整个数据集中的位置。 idx = slice(j * fold_size, (j + 1) * fold_size) # 这行代码根据切片对象 idx 从特征矩阵 X 和标签向量 y 中提取第 j 折的数据。 # X[idx, :] 表示提取特征矩阵 X 中索引范围为 idx 的所有行，所有列的数据。 # y[idx] 表示提取标签向量 y 中索引范围为 idx 的数据。 X_part, y_part = X[idx, :], y[idx] # 这是一个条件判断语句。 # 如果当前折的索引 j 等于指定的验证折索引 i，说明这一折将作为验证集。 if j == i: # 将第 j 折的数据作为验证集的特征和标签。 X_valid, y_valid = X_part, y_part # 如果训练集还未初始化（即 X_train 为 None），说明这是第一次添加数据到训练集。 elif X_train is None: # 将第 j 折的数据作为训练集的特征和标签。 X_train, y_train = X_part, y_part # 否则，说明训练集已经有部分数据，需要将当前折的数据添加到已有的训练集中。 else: # 这行代码使用了 torch.cat 函数，它用于将多个张量沿着指定的维度拼接在一起。 # [X_train, X_part] 是要拼接的张量列表，0 表示沿着第 0 维（样本维度）进行拼接。 # 这样就将当前折的特征数据添加到了训练集的特征数据中。 X_train = torch.cat([X_train, X_part], 0) # 同理，将当前折的标签数据添加到了训练集的标签数据中。 y_train = torch.cat([y_train, y_part], 0) # 最后，返回训练集的特征、训练集的标签、验证集的特征和验证集的标签。 return X_train, y_train, X_valid, y_valid 言简意赅的就是，K折交叉验证是将一个完整的数据分为k批次，然后依次选一个为验证集，其他为训练集（这就是为什么这个方法一定要写for循环），这个函数的的作用，就是提取出了哪些为验证级，哪些为数据集。最后返回训练集的特征、训练集的标签、验证集的特征和验证集的标签。\n然后看看train函数，train函数最具有借鉴价值，其中的许多结构都可以照搬不误\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size): # 初始化两个空列表，用于存储每个训练周期（epoch）结束后的训练损失和测试损失。 # 这些损失值将以对数均方根误差（log RMSE）的形式记录。 train_ls, test_ls = [], [] # 使用 d2l.load_array 函数创建一个训练数据的迭代器 train_iter。 # 该函数将训练特征和训练标签组合成一个数据集，并按照指定的批量大小进行划分。 # 这样，在训练过程中可以按批次加载数据，提高训练效率。 train_iter = d2l.load_array((train_features, train_labels), batch_size) # 使用 torch.optim.Adam 优化器来更新模型的参数。 # Adam 是一种常用的优化算法，结合了 AdaGrad 和 RMSProp 的优点。 # net.parameters() 表示要更新的模型参数，lr 是学习率，weight_decay 是权重衰减系数。 optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay) # 外层循环，控制训练的轮数，即模型对整个训练数据集进行多少次完整的训练。 for epoch in range(num_epochs): # 内层循环，遍历训练数据的每个批次。 # 在每个批次中，从 train_iter 中获取一批特征数据 X 和对应的标签数据 y。 for X, y in train_iter: # 在每次参数更新之前，需要将优化器中的梯度清零。 # 这是因为 PyTorch 会累积梯度，不清零会导致梯度错误地累加。 optimizer.zero_grad() # 前向传播：将输入数据 X 传入模型 net 中，得到模型的预测值。 # 然后使用预先定义的损失函数 loss 计算预测值与真实标签 y 之间的损失。 l = loss(net(X), y) # 反向传播：计算损失函数关于模型参数的梯度。 # 这一步会自动计算并存储每个参数的梯度值。 l.backward() # 优化器根据计算得到的梯度更新模型的参数。 # 具体的更新规则由优化器的算法决定，这里使用的是 Adam 算法。 optimizer.step() # 在每个训练周期结束后，计算并记录训练集的对数均方根误差（log RMSE）。 # log_rmse 函数会将模型的预测值进行裁剪，避免出现负数或零，然后计算对数均方根误差。 train_ls.append(log_rmse(net, train_features, train_labels)) # 如果提供了测试数据的标签，则计算并记录测试集的对数均方根误差（log RMSE）。 if test_labels is not None: test_ls.append(log_rmse(net, test_features, test_labels)) # 最后返回训练集和测试集（如果有）在每个训练周期结束后的对数均方根误差列表。 return train_ls, test_ls 对于这个函数，我们总结这样的套路\ntrain_iter必不可少\ntrain_iter=d2l.load_array((train_features,train_labels),batch_size) 这一行姑且看作是固定的代码，有了合格datalodar的数据类型，才方便进行进一步的分组训练\n但是你以后是肯定不用d2l库的（这就是一个大槽点），所以用pytorch自己的库文件就给这样写\n1 2 3 4 5 6 7 8 from torch.utils.data import TensorDataset, DataLoader # 原代码 # train_iter = d2l.load_array((train_features, train_labels), batch_size) # 替换后 train_dataset = TensorDataset(train_features, train_labels) train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) 选择一个优化器\noptimizer=torch.optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n建立for循环训练\n其实如何更新参数的具体过程，已经内置在了torch里面，这里其实是看不见的 X是特征，y是标签\nfor epoch in range(num_epochs):\rfor X,y in train_iter:\roptimizer.zero_grad()\rl=loss(net(X),y)\rl.backward()\roptimizer.step()\rtrain_ls.append(log_rmse(net,train_features,train_labels))\rif test_labels is not None:\rtest_ls.append(log_rmse(net,test_features,test_labels))\r此函数最后放回的是训练集和测试集的均方根误差，用于衡量这个效果 同时net也训练好了，优化器自动做好了这些事情\nk_fold函数\n解释了上面两个函数之后，k_fold函数就不难了，返回的是k次验证的平均损失值，这里不再讲解\ntrain_and_pred函数\n简称来说，这一步就是在之前已经完成了net的训练之后，再一次传入test_data，然后回放回相应的标签 但是要注意，这里这个方法是预测出的价值，只用于回归问题，不可以用于分类问题 preds = net(test_features).detach().numpy()\r这里detch方法和numpy都直接计吧 如果要用分类问题的话，对于二分类，可以用 preds_bool = preds \u0026gt;= 0.5\r最后保存成csv文件提交就好了\n1 2 3 test_data[\u0026#39;SalePrice\u0026#39;] = pd.Series(preds.reshape(1, -1)[0]) submission = pd.concat([test_data[\u0026#39;Id\u0026#39;], test_data[\u0026#39;SalePrice\u0026#39;]], axis=1) submission.to_csv(\u0026#39;submission.csv\u0026#39;, index=False) ","date":"2025-07-15T00:00:00Z","permalink":"https://junyu-chen-biter.github.io/p/kaggle%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"kaggle学习笔记"},{"content":"关于yaml的配置 这个文件可以修改网站的一些配置\ncopyright就是人的名字\n语言里面的title是我的名字\ncustomText是自定义文本\nenabled是评论，我其实先关掉了\nwidgets是装饰，可以自己选择关不关\nsocial是侧边栏的搜索，可以自己改\n在cmd中用命令，\nhugo new content post/博客名/index.md\n这样来创建内容\n在themes的stack里面有个样例，那里面可以有一些样例文件，可以查看 还有title是在文件里面直接改的\n还有，如果要在同一文件下引用图片，md文件的名字只能命名为index\n1 2 3 4 5 6 7 8 title: Chinese Test description: 这是一个副标题 date: 2020-09-09 slug: test-chinese image: helena-hertz-wWZzXlDpMog-unsplash.jpg categories: - Test - 测试 这上面就是样例可以改的地方，其中categories的地方就是旁边的标签，img是封面\n这个方法中。draft的状态还会影响是否显示，乐了\n目前网站还是手动部署，每一次写完以后，用hugo -D（在dev目录下）重新生成文件，之后进入public文件，\ngit add . git commit -m \u0026ldquo;更新xx年xx日\u0026rdquo; git push origin master\n然后在github上pull resquest，合并更新\n这个方法非常愚蠢，不是很好，因为有几次失误的操作导致我的main分支不知道去哪了（）\n","date":"2025-07-06T16:14:48+08:00","permalink":"https://junyu-chen-biter.github.io/p/%E7%BD%91%E7%AB%99%E6%90%AD%E5%BB%BA%E8%AF%B4%E6%98%8E/","title":"网站搭建说明"},{"content":"京西南 大一小记 时间真是太快了，没想到短短一年的时间，大一就结束了，最后取得成绩还行，写此文的时候第二学期的成绩还没出来，所以在第一学期综测rank5%，纯成绩rank10%，第二学期纯纯被淑芬被刺，总rank没出来，但是应该不如上学期（），在这里写一些我踩过的坑，供后人参考，当然其中也有我不切实际的幻想，唉唉等未来再看吧\n题目名意为：北京西南边的良乡大一小记\n学习篇 推荐几个网站：\nhttps://www.tboxn.com/sites/320.html\nhttps://survivesjtu.gitbook.io/survivesjtumanual\n数学分析1 大一上学期最重要的课，十分偏证明。大一的课程很多，而且有军事理论军事技巧，并且24届批分出来之后分差不大，没出现5%比20%高10多分的情况，所以尽量好好学，保障在前25%就不会拉总均分。我在学这个的时候，依然在保留纸质学习，真是一个大坑，尽早用无纸化学习更好\n线性代数 长远来看，这是大一最重要的课，然而我炸了，十分不适应老师的讲课风格，去找了一个全英文的网课看，而却完全忽略了课本，最后只考了一个均分，分析下来是计算太差了，同时不能完全不看课本。最后挺好应试的，但是只学到应试那个难度的话，对以后的科研没啥帮助，我就后悔一直以一个应试的态度学这门课，平时也是只做好课后习题。线性代数完全值得你去找不同的课本反复学习多遍，理解其中的原理是最好的。\nC语言 很看发挥啊，主要就是要提前备考期末会考的那几个算法，递归、链表、字符串、打印图形。其实挺没意思的，这么考最后比的是应试能力，一堆oi算法都没涉及，会导致很多学生平时的努力无法反映在考试中，但是可以借此探索一下自己的编程水平。我其实考寄了，但是老师大捞特捞，最后评分也不差\n人工智能and生科 放在一起说明这两个课就是纯纯的背书，平时一点不用听，考前看PPT和资料完全足够。人工智能这个课，虽然吐槽的很多，但是客观上来说确实讲了许多ai的知识，是一个较为广泛的科普\n思政and习概and史纲 这个主要看老师和平时分，想卷高分平时分一定要拉满！考试的话，思政看小红书的知识点，习概看老师的重点，史纲emmmm看命。\n数学分析2 真正的淑芬王朝！，大一下想要均分高，淑芬二是关键中的关键！！！因为分差极大。我大一下每一科的排名，综合来看是要比大一上好（因为大一上线代寄了），但是淑芬二却让我总排名不如大一上。就是因为大一下的学分少，淑芬占比高而且分差大，其他科目难以拉开分，我74分，均分64，然而随便来一个淑芬上80的佬，就可以给我拉开一大堆均分。大一下的总成绩排名，也是和淑芬高度相关。如果想卷，这个科目甚至可以寒假先开始学\n大物1 没啥可说的，纯纯老本，我觉得定期复习加上总结一下公式就行了，可以照搬高中的方法，难度不大，但是4学分，还是尽量冲高分\n机械制图 纯纯折磨，24届的几位伟大学长给小灯制作了完整的题库，非常有用。这个科目，基本就是看PPT加刷题，平时的作业一定好好做。至于听课，因人而异吧。\n智能机电实践 队友大于一切，本人幸运地遇见大佬队友，最后97分拿下此课，没啥技巧，祈祷自己遇见好的队友吧。哦哦，这个课程很锻炼ai使用能力，如果愿意深入了解，其实是一个很好的增强自己实践能力的契机，也是一个很好的偏自动化控制算法的入门，态度好是真能学到东西\n大学物理实验 难评价，给分有点看运气，想要高分，尽量确保自己绪论部分拿满，那是真的自己可以改变的部分\n总结 数学类\n数学分析+线性代数=16学分\n思政类\n习概+史纲+思政+国安+行测=10.5学分\n物理类 大物1=4学分\n大学新内容 C语言+机械制图+智能机电+\n课外事物 大创 其实可以算参加了两个，但是都不是核心成员，而且成绩也未知。这个东西，想搞的话就去跟老师（然后你就会被国内高校界的挂名包装之风污染灵魂）。很难评，有些大创真的做东西，比赛过程中甚至会产出论文和专利，有的就是纯纯做PPT。所以建议找老师做，老师会提供一些资源。但是很多大创都是把老师的项目套给学生，这就看自己能不能接受了\n比赛 算法类的唯一算得上成就的是PAT三等奖（好吧，也是水赛），oi类的比赛对于无编程基础的小灯基本可以劝退。含金量高、无挂名可能的背后，也是其很低的收益付出比，非oier想卷计算机还有很多其他赛道可以选择\n建模类无，参加了一个统计建模，了解了大概是个啥流程，之后感觉不感兴趣，就没有坚持。个人觉得含金量一般（包括国赛），没听见在什么地方这个比赛是很必要的\nrm，rc无，我就没参加这些队伍，乐。\n学生事物 组织加了个记者团和学生会，社团是辩论社。大一上确实花了些时间在辩论上，最后拿了院级亚军，感觉辩论给人的影响还是正面的，也可以认识许多人，是段不错的经历。学生组织的话也主要是认识了一些朋友，不过我大一下就基本没弄这些了\n班委当的是班长，相比于团支书，我真是十分悠闲的了\n实习and科研 zero，彻底没开始\n推荐篇 《金榜题名之后：大学生出路分化之谜》这个书值得一看，至少给了我很多启发\n纯懒，慢慢更新\n","date":"2025-07-06T00:00:00Z","image":"https://junyu-chen-biter.github.io/p/%E4%BA%AC%E8%A5%BF%E5%8D%97%E5%A4%A7%E4%B8%80%E5%B0%8F%E8%AE%B0/farm_hu_4b05d451a45eb568.jpeg","permalink":"https://junyu-chen-biter.github.io/p/%E4%BA%AC%E8%A5%BF%E5%8D%97%E5%A4%A7%E4%B8%80%E5%B0%8F%E8%AE%B0/","title":"京西南大一小记"},{"content":"这是音乐 其实也只有自己能懂，那些音乐背后的回忆才是最珍贵的，这东西也不能分享，于是写在此处（暴露乐品）\n于是这便是一章随机，听到啥想记的，就随手记下来吧，可以也就当作一个音乐封面的记录好了\n2025-7-6，于假期，考科目三前（最火考过了）\n2025-7-6，于假期，搭建网站时\n","date":"2025-07-06T00:00:00Z","image":"https://junyu-chen-biter.github.io/p/%E9%82%A3%E4%BA%9B%E6%AD%8C%E6%9B%B2/song1-unsplash_hu_f3664672da805c1a.jpg","permalink":"https://junyu-chen-biter.github.io/p/%E9%82%A3%E4%BA%9B%E6%AD%8C%E6%9B%B2/","title":"那些歌曲"}]